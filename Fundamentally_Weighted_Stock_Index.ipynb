{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedaa434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf7e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"NFLX\", \"AMC\", \n",
    "                    \"NVDA\", \"META\", \"BRK-B\", \"V\", \"JPM\", \"UNH\", \"PG\", \"HD\", \n",
    "                    \"DIS\", \"VZ\", \"MA\", \"INTC\", \"CMCSA\", \"PEP\", \"KO\", \"MRK\", \n",
    "                    \"T\", \"CSCO\", \"XOM\", \"ABT\", \"NKE\", \"PFE\", \"CVX\", \"T\", \"TMUS\"]\n",
    "def data_collection(tickers):\n",
    "    data = {}\n",
    "    for ticker in tickers:\n",
    "        info = yf.Ticker(ticker).info\n",
    "        data[ticker] = {\"Ticker\": ticker,\n",
    "                        \"Name\": info.get(\"longName\", None),\n",
    "                        \"price\": info.get(\"currentPrice\", None),\n",
    "                        \"market_cap\": info.get(\"marketCap\", None),\n",
    "                        \"volatility\": info.get(\"beta\", None),\n",
    "                        \"dividend_yield\": info.get(\"dividendYield\", None),\n",
    "                        \"revenue\": info.get(\"totalRevenue\", None),\n",
    "                        \"pre_ratio\": info.get(\"trailingPE\", None),\n",
    "                        \"roe\": info.get(\"returnOnEquity\", None),\n",
    "                        \"de_ratio\": info.get(\"debtToEquity\", None)}\n",
    "    df = pd.DataFrame(data).T\n",
    "    return df\n",
    "\n",
    "required_data = data_collection(required_tickers)\n",
    "required_data = required_data.dropna()\n",
    "required_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f77551",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_data = pd.read_excel(\"Analysis sheet-Indxx 500 Index~INXT Final.xlsx\")\n",
    "index_data = pd.read_excel('Analysis sheet-Indxx 500 Index~INXT Final.xlsx', sheet_name = 'Analysis sheet-Indxx 500 Index~', parse_dates=['date'])\n",
    "index_data['date'] = pd.to_datetime(index_data['date'], format='%m/%d/%Y', errors='coerce')\n",
    "index_data = index_data.dropna(subset=['Rebase value']) # Dropping rows where 'Rebase value' is NaN\n",
    "index_data = index_data.rename(columns={'Rebase value': 'index_value'})\n",
    "# index_data = index_data.rename(columns={'Rebase value': 'required_value'})\n",
    "index_data = index_data.sort_values('date').reset_index(drop=True)\n",
    "print(index_data)\n",
    "\n",
    "# Simulating returns for the required tickers\n",
    "index_data['return'] = np.log(index_data['index_value'] / index_data['index_value'].shift(1))\n",
    "mean_return_daily = index_data['return'].mean()\n",
    "mean_return_annualized = mean_return_daily * 252  # 252 is trading days/year\n",
    "print(\"Mean annualized return:\", mean_return_annualized)\n",
    "volatility_daily = index_data['return'].std()\n",
    "volatility_annualized = volatility_daily * np.sqrt(252)\n",
    "print(\"Annualized volatility:\", volatility_annualized)\n",
    "total_return = index_data['value'].iloc[-1] / index_data['value'].iloc[0] - 1\n",
    "print(\"Total return:\", total_return)\n",
    "risk_free_rate_annual = 0.03      # Change as per your requirement\n",
    "risk_free_rate_daily = risk_free_rate_annual / 252\n",
    "# Sharpe ratio (annualized)\n",
    "excess_daily = index_data['return'] - risk_free_rate_daily\n",
    "sharpe_ratio_annualized = (excess_daily.mean() * 252) / (index_data['return'].std() * np.sqrt(252))\n",
    "print(\"Annualized Sharpe Ratio:\", sharpe_ratio_annualized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee929ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Historical Data\n",
    "def get_historical_data(tickers, start_date, end_date):\n",
    "    data = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
    "    data = data.dropna(axis=1, how='all')\n",
    "    data.to_excel(\"Historical_Data_Daily.xlsx\", index=True)  # Save to Excel\n",
    "    return data\n",
    "\n",
    "historical_data = get_historical_data(required_tickers, \"2018-04-30\", \"2025-06-05\")\n",
    "print(historical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10530474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rebasing Historical Stock Prices\n",
    "def rebase_stock_prices(df, base_date='2018-04-30', base_value=1000):\n",
    "    df = df.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    rebased = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "    rebased.loc[base_date] = base_value\n",
    "    for stock in df.columns:\n",
    "        prices = df[stock]\n",
    "        rebased_stock = pd.Series(index=prices.index, dtype='float64')\n",
    "        rebased_stock.loc[base_date] = base_value\n",
    "        for i in range(prices.index.get_loc(base_date)+1, len(prices)):\n",
    "            today = prices.index[i]\n",
    "            prev = prices.index[i-1]\n",
    "            # Rebase rule: Value_today = Value_prev * (Price_today / Price_prev)\n",
    "            rebased_stock[today] = rebased_stock[prev] * (prices[today] / prices[prev])\n",
    "        rebased[stock] = rebased_stock\n",
    "    \n",
    "    rebased = rebased.astype(float).reset_index()\n",
    "    return rebased\n",
    "\n",
    "rebased_data = rebase_stock_prices(historical_data.reset_index())\n",
    "rebased_data.to_excel(\"Rebased_Stock_Prices.xlsx\", index=False)  # Save to Excel\n",
    "print(rebased_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f39a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find intersection of tickers present in all datasets\n",
    "tickers_in_data = set(required_data.index)\n",
    "tickers_in_hist = set(historical_data.columns)\n",
    "common_tickers = list(tickers_in_data & tickers_in_hist)\n",
    "\n",
    "# Filter all data to only use common tickers\n",
    "required_tickers = sorted(list(common_tickers))\n",
    "required_data = required_data.loc[required_tickers]\n",
    "historical_data = historical_data[required_tickers]\n",
    "np.random.seed(42)  # For reproducibility\n",
    "simulated_returns = np.random.normal(0.01, 0.03, (5, len(required_tickers)))  # 5 periods, n stocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5493486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Equally Weighted Portfolio\n",
    "def equally_weighted_portfolio(df):\n",
    "    n = df.shape[0]\n",
    "    weights = np.ones(n) / n\n",
    "    df[\"Equal Weighted Portfolio\"] = weights\n",
    "    return df\n",
    "\n",
    "equally_weighted_portfolio(required_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f321e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Price Weight Portfolio \n",
    "def price_weighted_portfolio(df):\n",
    "    total_price = df[\"price\"].sum()\n",
    "    df[\"Price Weighted Portfolio\"] = df[\"price\"] / total_price\n",
    "    return df\n",
    "\n",
    "price_weighted_portfolio(required_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2175dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Market Cap Weighted Portfolio\n",
    "def market_cap_weighted_portfolio(df):\n",
    "    total_market_cap = df[\"market_cap\"].sum()\n",
    "    df[\"Market Cap Weighted Portfolio\"] = df[\"market_cap\"] / total_market_cap\n",
    "    return df\n",
    "\n",
    "market_cap_weighted_portfolio(required_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Fundamental Composite Weighting\n",
    "def fundamental_composite_weighted_portfolio(df):\n",
    "    df[\"roe_score\"] = (df[\"roe\"]-df[\"roe\"].min())/(df[\"roe\"].max()-df[\"roe\"].min())\n",
    "    df[\"pe_score\"] = 1-(df[\"pre_ratio\"]-df[\"pre_ratio\"].min())/(df[\"pre_ratio\"].max()-df[\"pre_ratio\"].min()) #Price to Earnings Ratio\n",
    "    df[\"de_score\"] = 1-(df[\"de_ratio\"]-df[\"de_ratio\"].min())/(df[\"de_ratio\"].max()-df[\"de_ratio\"].min()) #Debt to Equity Ratio\n",
    "    Score = (0.4*df[\"roe_score\"]+0.3*df[\"pe_score\"]+0.3*df[\"de_score\"])\n",
    "    df[\"Fundamental Composite Weighting\"] = Score / Score.sum()\n",
    "    return df\n",
    "\n",
    "fundamental_composite_weighted_portfolio(required_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Inverse Volatility Weighting\n",
    "def inverse_volatility_weighted_portfolio(df):\n",
    "    inverse_volatility = 1/df[\"volatility\"]\n",
    "    df[\"Inverse Volatility Weight\"] = inverse_volatility / inverse_volatility.sum()\n",
    "    return df\n",
    "\n",
    "inverse_volatility_weighted_portfolio(required_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Minimum Variance Portfolio\n",
    "def minimum_variance_portfolio(df, tickers):\n",
    "    df = df.loc[tickers].copy()\n",
    "    cov_matrix = np.cov(simulated_returns, rowvar=False)\n",
    "    if cov_matrix.shape[0] != len(tickers):\n",
    "        raise ValueError(f\"Covariance matrix shape {cov_matrix.shape} does not match number of tickers {len(tickers)}\")\n",
    "    w_minvar = cp.Variable(len(tickers))\n",
    "    prob = cp.Problem(cp.Minimize(cp.quad_form(w_minvar, cov_matrix)),\n",
    "                      [cp.sum(w_minvar) == 1, w_minvar >= 0])\n",
    "    prob.solve()\n",
    "    w_minvar_sol = w_minvar.value / w_minvar.value.sum()\n",
    "    df[\"Minimum Variance Portfolio\"] = w_minvar_sol\n",
    "    return df\n",
    "\n",
    "required_data = minimum_variance_portfolio(required_data, required_tickers)\n",
    "required_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Maximum Sharpe Ratio Portfolio\n",
    "def maximum_sharpe_ratio_portfolio(df, tickers):\n",
    "    n = len(tickers)\n",
    "    w = cp.Variable(n)\n",
    "    cov_matrix = historical_data.cov().values\n",
    "    avg_returns = simulated_returns.mean(axis=0)\n",
    "    target_return = avg_returns.mean()\n",
    "    constraints = [cp.sum(w) == 1, w >= 0, avg_returns @ w >= target_return]\n",
    "    portfolio_variance = cp.quad_form(w, cov_matrix)\n",
    "    problem = cp.Problem(cp.Minimize(portfolio_variance), constraints)\n",
    "    print(\"DCP compliant?\", problem.is_dcp())\n",
    "    problem.solve()\n",
    "    optimal_weights = w.value\n",
    "    weights_series = pd.Series(optimal_weights, index=df.index)\n",
    "    df[\"Maximum Sharpe Ratio Portfolio\"] = weights_series\n",
    "    return df\n",
    "\n",
    "maximum_sharpe_ratio_portfolio(required_data, required_tickers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407b8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Risk Parity Portfolio\n",
    "def risk_parity_portfolio(df, tickers):\n",
    "    n = len(tickers)\n",
    "    simulated_returns = np.random.normal(0.01, 0.03, (5, n))  # Simulated returns for n stocks\n",
    "    cov_matrix = np.cov(simulated_returns, rowvar=False)\n",
    "    risk_contribution = lambda w, cov: w * (cov @ w) / np.sqrt(w @ cov @ w)\n",
    "    w_risk_parity = np.ones(n) / n\n",
    "    for _ in range(100): \n",
    "        rc = risk_contribution(w_risk_parity, cov_matrix)\n",
    "        target = rc.mean()\n",
    "        w_risk_parity *= target / rc\n",
    "        w_risk_parity /= w_risk_parity.sum()\n",
    "    weights_series = pd.Series(w_risk_parity, index=tickers)\n",
    "    df = df.loc[tickers]\n",
    "    df[\"Risk Parity Portfolio\"] = weights_series\n",
    "    return df\n",
    "\n",
    "required_data = risk_parity_portfolio(required_data, required_tickers)\n",
    "required_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed51b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Dividend Yield Weighted Portfolio\n",
    "def dividend_yield_weighted_portfolio(df):\n",
    "    dividend_yield_sum = df[\"dividend_yield\"].sum()\n",
    "    weight_dy = df[\"dividend_yield\"] / dividend_yield_sum\n",
    "    weight_dy = weight_dy.fillna(0)  # Fill NaN values with 0\n",
    "    df[\"Dividend Yield Weighted Portfolio\"] = weight_dy\n",
    "    return df\n",
    "\n",
    "dividend_yield_weighted_portfolio(required_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. Revenue Weighted Portfolio\n",
    "def revenue_weighted_portfolio(df):\n",
    "    revenue_sum = df[\"revenue\"].sum()\n",
    "    weight_rev = df[\"revenue\"] / revenue_sum\n",
    "    weight_rev = weight_rev.fillna(0)  # Fill NaN values with 0\n",
    "    df[\"Revenue Weighted Portfolio\"] = weight_rev\n",
    "    return df\n",
    "revenue_weighted_portfolio(required_data)\n",
    "\n",
    "# required_data.to_excel(\"Weights.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d1ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_data.to_excel(\"Sample Portfolio.xlsx\", index=True, sheet_name = \"Weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba71b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighing method comparison\n",
    "weighting_methods = [\n",
    "    (\"Equal Weighted Portfolio\", equally_weighted_portfolio(required_data)[\"Equal Weighted Portfolio\"].values),\n",
    "    (\"Price Weighted Portfolio\", price_weighted_portfolio(required_data)[\"Price Weighted Portfolio\"].values),\n",
    "    (\"Market Cap Weighted Portfolio\", market_cap_weighted_portfolio(required_data)[\"Market Cap Weighted Portfolio\"].values),\n",
    "    (\"Fundamental Composite Weighting\", fundamental_composite_weighted_portfolio(required_data)[\"Fundamental Composite Weighting\"].values),\n",
    "    (\"Inverse Volatility Weight\", inverse_volatility_weighted_portfolio(required_data)[\"Inverse Volatility Weight\"].values),\n",
    "    (\"Minimum Variance Portfolio\", minimum_variance_portfolio(required_data, required_tickers)[\"Minimum Variance Portfolio\"].values),\n",
    "    (\"Maximum Sharpe Ratio Portfolio\", maximum_sharpe_ratio_portfolio(required_data, required_tickers)[\"Maximum Sharpe Ratio Portfolio\"].values),\n",
    "    (\"Risk Parity Portfolio\", risk_parity_portfolio(required_data, required_tickers)[\"Risk Parity Portfolio\"].values),\n",
    "    (\"Dividend Yield Weighted Portfolio\", dividend_yield_weighted_portfolio(required_data)[\"Dividend Yield Weighted Portfolio\"].values),\n",
    "    (\"Revenue Weighted Portfolio\", revenue_weighted_portfolio(required_data)[\"Revenue Weighted Portfolio\"].values)\n",
    "]\n",
    "# Plotting the weights of each method\n",
    "plt.figure(figsize=(12, 8))\n",
    "for method, weights in weighting_methods:\n",
    "    plt.plot(required_data.index, weights, label=method)\n",
    "plt.title(\"Weights of Different Portfolio Weighting Methods\")\n",
    "plt.xlabel(\"Stocks\")\n",
    "plt.ylabel(\"Weights\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Output.png\", dpi = 1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_portfolios(weights_file, prices_file):\n",
    "    df_weights = pd.read_excel(weights_file, sheet_name='Weights')\n",
    "    df_prices = pd.read_excel(prices_file, sheet_name='Sheet1')\n",
    "    weight_categories = [\n",
    "        'Equal Weighted Portfolio',\n",
    "        'Price Weighted Portfolio',\n",
    "        'Market Cap Weighted Portfolio',\n",
    "        'Fundamental Composite Weighting',\n",
    "        'Inverse Volatility Weight', \n",
    "        'Minimum Variance Portfolio',\n",
    "        'Maximum Sharpe Ratio Portfolio', \n",
    "        'Risk Parity Portfolio',\n",
    "        'Dividend Yield Weighted Portfolio',\n",
    "        'Revenue Weighted Portfolio'\n",
    "    ]\n",
    "    # Ensure 'Date' is in datetime format\n",
    "    df_prices['Date'] = pd.to_datetime(df_prices['Date'], errors='coerce')\n",
    "    # df_prices = df_prices.dropna(subset=['Date'])  # Drop rows with NaT dates\n",
    "    df_prices = df_prices.sort_values('Date')  # Sort by date\n",
    "    df_prices = df_prices.reset_index(drop=True)\n",
    "    \n",
    "    stocks = df_weights['Ticker'].values\n",
    "    df_prices = df_prices[stocks]\n",
    "    \n",
    "    # Prepare weights as DataFrame indexed by Ticker\n",
    "    weights_data = df_weights.set_index('Ticker')[weight_categories]\n",
    "    \n",
    "    # Calculate daily weighted portfolio values for each category\n",
    "    weighted_portfolios = {}\n",
    "    for category in weight_categories:\n",
    "        weights = weights_data[category]\n",
    "        weighted_sum = df_prices.mul(weights, axis=1).sum(axis=1)\n",
    "        weighted_portfolios[category] = weighted_sum\n",
    "    weighted_portfolios_df = pd.DataFrame(weighted_portfolios)\n",
    "    weighted_portfolios_df.to_excel('Weighted_Portfolios.xlsx', index=False)\n",
    "    return weighted_portfolios_df\n",
    "\n",
    "calculate_weighted_portfolios('Sample Portfolio.xlsx', 'Rebased_Stock_Prices.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57daa2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighing method comparison with Indxx 500 Index Levels\n",
    "weighted_p = pd.read_excel('Weighted_Portfolios.xlsx')\n",
    "weighting_methods = [\n",
    "    (\"Equal Weighted Portfolio\", equally_weighted_portfolio(required_data)[\"Equal Weighted Portfolio\"].values),\n",
    "    (\"Price Weighted Portfolio\", price_weighted_portfolio(required_data)[\"Price Weighted Portfolio\"].values),\n",
    "    (\"Market Cap Weighted Portfolio\", market_cap_weighted_portfolio(required_data)[\"Market Cap Weighted Portfolio\"].values),\n",
    "    (\"Fundamental Composite Weighting\", fundamental_composite_weighted_portfolio(required_data)[\"Fundamental Composite Weighting\"].values),\n",
    "    (\"Inverse Volatility Weight\", inverse_volatility_weighted_portfolio(required_data)[\"Inverse Volatility Weight\"].values),\n",
    "    (\"Minimum Variance Portfolio\", minimum_variance_portfolio(required_data, required_tickers)[\"Minimum Variance Portfolio\"].values),\n",
    "    (\"Maximum Sharpe Ratio Portfolio\", maximum_sharpe_ratio_portfolio(required_data, required_tickers)[\"Maximum Sharpe Ratio Portfolio\"].values),\n",
    "    (\"Risk Parity Portfolio\", risk_parity_portfolio(required_data, required_tickers)[\"Risk Parity Portfolio\"].values),\n",
    "    (\"Dividend Yield Weighted Portfolio\", dividend_yield_weighted_portfolio(required_data)[\"Dividend Yield Weighted Portfolio\"].values),\n",
    "    (\"Revenue Weighted Portfolio\", revenue_weighted_portfolio(required_data)[\"Revenue Weighted Portfolio\"].values)\n",
    "]\n",
    "index500_stats = {\n",
    "    \"Mean Return\": index_data['return'].mean()*252,  # Annualized mean return\n",
    "    \"Volatility\": index_data['return'].std() * np.sqrt(252),  # Annualized volatility\n",
    "    \"Sharpe Ratio\": (index_data['return'].mean() - risk_free_rate_daily) / (index_data['return'].std() * np.sqrt(252)),  # Annualized Sharpe Ratio\n",
    "    \"Total Return\": (index_data['index_value'].iloc[-1] / index_data['index_value'].iloc[0]) - 1  # Total return\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame({method: weights for method, weights in weighting_methods}, index=required_data.index)\n",
    "results_df['Indxx 500 Total Return'] = index500_stats[\"Total Return\"]\n",
    "# To highlight the best method's result alongside the index's:\n",
    "last_weighted = weighted_p.tail(1).iloc[0]\n",
    "last_index_value = index_data['index_value'].iloc[-1]\n",
    "outperforming_portfolios = last_weighted[last_weighted > last_index_value]\n",
    "\n",
    "print(\"Index Value:\", last_index_value)\n",
    "print(\"\\nPortfolios outperforming the index:\")\n",
    "print(outperforming_portfolios)\n",
    "\n",
    "if not outperforming_portfolios.empty:\n",
    "    best_portfolio = outperforming_portfolios.idxmax()\n",
    "    best_value = outperforming_portfolios.max()\n",
    "    print(f\"\\nBest performing portfolio: {best_portfolio} with value {best_value}\")\n",
    "else:\n",
    "    print(\"\\nNo portfolio outperformed the index.\")\n",
    "\n",
    "results_df['Best Method Result'] = results_df[best_portfolio]\n",
    "results_df['Best Method Name'] = best_portfolio\n",
    "results_df['Best Method Result'] = results_df[best_portfolio]\n",
    "print(results_df)\n",
    "with pd.ExcelWriter('Sample Portfolio.xlsx', mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "    results_df.to_excel(writer, sheet_name='Portfolio Results')\n",
    "    index_data.to_excel(writer, sheet_name='Index Data')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
